{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "from enum import Enum\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from local_landmark import LocalLandmark\n",
    "from realsense_camera import RealSenseCamera\n",
    "from yolo_object import YoloObject\n",
    "from hand_helper import HandHelper\n",
    "from object_tracker import ObjectTracker\n",
    "from robot_controller import RobotController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_FPS = 10\n",
    "SAMPLING_SECONDS = 1\n",
    "PREDICTION_WINDOW_SIZE = 5\n",
    "sequence_length = int(SAMPLING_SECONDS*SAMPLING_FPS)\n",
    "frame_interval = 1.0/SAMPLING_FPS\n",
    "\n",
    "IMAGE_WIDTH = 640\n",
    "IMAGE_HEIGHT = 480\n",
    "\n",
    "HAND_CONNECTIONS = ((0, 1), (0, 5), (9, 13), (13, 17), (5, 9), (0, 17), (1, 2), (2, 3), (3, 4), (5, 6), (6, 7), (7, 8),\n",
    "                    (9, 10), (10, 11), (11, 12), (13, 14), (14, 15), (15, 16), (17, 18), (18, 19), (19, 20))\n",
    "\n",
    "class Action(Enum):\n",
    "    IDLE = 0\n",
    "    PICK = 1\n",
    "    PLACE = 2\n",
    "    SCREW_WRENCH = 3\n",
    "\n",
    "OBJECT_NAMES = {\n",
    "    0: \"small_screw\",\n",
    "    1: \"big_screw\",\n",
    "    2: \"small_wrench\",\n",
    "    3: \"big_wrench\",\n",
    "    4: \"cap\",\n",
    "    5: \"barrel\",\n",
    "    6: \"piston\",\n",
    "    7: \"support\",\n",
    "    8: \"air_connector\",\n",
    "    9: \"nut\"\n",
    "}\n",
    "\n",
    "num_classes_actions = len(Action)\n",
    "num_classes_objects = len(OBJECT_NAMES) + 1 # Last class for empty hand\n",
    "\n",
    "label_colors = [np.random.random(3)*255 for _ in range(len(OBJECT_NAMES))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "yolo_model_path = os.path.join(\"weights\", \"yolov9c_fine_tuned.pt\")\n",
    "yolo_model = YOLO(yolo_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks_from_flattened_array(flattened_landmarks):\n",
    "    N_RIGHT_HAND_LANDMARKS = 21\n",
    "    N_LEFT_HAND_LANDMARKS = 21\n",
    "\n",
    "    right_hand_landmarks = []\n",
    "    left_hand_landmarks = []\n",
    "\n",
    "    cursor = 0\n",
    "\n",
    "    for _ in range(N_RIGHT_HAND_LANDMARKS):\n",
    "        cursor_end_position = cursor + 4\n",
    "        lm_sub_array = flattened_landmarks[cursor:cursor_end_position]\n",
    "        right_hand_landmarks.append(LocalLandmark.from_np_array(lm_sub_array))\n",
    "        cursor = cursor_end_position\n",
    "\n",
    "    for _ in range(N_LEFT_HAND_LANDMARKS):\n",
    "        cursor_end_position = cursor + 4\n",
    "        lm_sub_array = flattened_landmarks[cursor:cursor_end_position]\n",
    "        left_hand_landmarks.append(LocalLandmark.from_np_array(lm_sub_array))\n",
    "        cursor = cursor_end_position\n",
    "\n",
    "    return right_hand_landmarks, left_hand_landmarks\n",
    "\n",
    "def one_hot_encode_class(class_index, num_classes):\n",
    "    one_hot_vector = np.zeros(num_classes, dtype=int)\n",
    "    one_hot_vector[class_index] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def get_selected_frame_data(frame_landmarks_data, frame_objects_data):\n",
    "    selected_frame_data = []\n",
    "\n",
    "    right_hand_landmarks, left_hand_landmarks = get_landmarks_from_flattened_array(frame_landmarks_data)\n",
    "    for i in range(2):\n",
    "        if i == 0:\n",
    "            hand_landmarks = right_hand_landmarks\n",
    "        else:\n",
    "            continue # Select only the right hand\n",
    "            hand_landmarks = left_hand_landmarks\n",
    "            \n",
    "        wrist_landmark = hand_landmarks[0]\n",
    "        for lm in hand_landmarks:\n",
    "            # Absolute coordinates (with respect to camera frame)\n",
    "            selected_frame_data.append(lm.x)\n",
    "            selected_frame_data.append(lm.y)\n",
    "            # selected_frame_data.append(lm.d/IMAGE_WIDTH)\n",
    "\n",
    "            # Relative coordinates (relative to the wrist landmark)\n",
    "            selected_frame_data.append(lm.x - wrist_landmark.x)\n",
    "            selected_frame_data.append(lm.y - wrist_landmark.y)\n",
    "            selected_frame_data.append(lm.z)\n",
    "\n",
    "    if frame_objects_data is not None:\n",
    "        right_hand_object_found = False\n",
    "        for frame_object_data in frame_objects_data:\n",
    "            label_id, in_hand, x1, y1, x2, y2 = frame_object_data\n",
    "            if in_hand == 0: # Only objects held in the right hand are considered\n",
    "                selected_frame_data = np.concatenate((selected_frame_data, one_hot_encode_class(label_id, num_classes_objects)), axis=0)\n",
    "                right_hand_object_found = True\n",
    "                break\n",
    "        \n",
    "        if not right_hand_object_found:\n",
    "            selected_frame_data = np.concatenate((selected_frame_data, one_hot_encode_class(num_classes_objects - 1, num_classes_objects)), axis=0)\n",
    "\n",
    "    return selected_frame_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_landmarks(frame, model):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame.flags.writeable = False\n",
    "    results = model.process(frame)\n",
    "    return results\n",
    "\n",
    "def extract_hand_landmarks(hand_index, multi_hand_landmarks, multi_handedness, extract_depth=False, camera=None):\n",
    "    \"\"\"hand_index: 0 = right, 1 = left\"\"\"\n",
    "    hand_landmarks_list = []\n",
    "    hand_index_in_multi_landmarks = -1\n",
    "    if multi_hand_landmarks and multi_handedness:\n",
    "        for possible_hand_index in range(len(multi_hand_landmarks)):\n",
    "            handedness_classification = multi_handedness[possible_hand_index].classification[0]\n",
    "            handedness_index = handedness_classification.index\n",
    "            handedness_score = handedness_classification.score\n",
    "            if handedness_index == hand_index: # and handedness_score > 0.7\n",
    "                hand_index_in_multi_landmarks = possible_hand_index\n",
    "                break\n",
    "\n",
    "    for i in range(21):\n",
    "        if hand_index_in_multi_landmarks >= 0:\n",
    "            lm = LocalLandmark.from_mediapipe_hand_landmark(multi_hand_landmarks[hand_index_in_multi_landmarks].landmark[i])\n",
    "            if extract_depth:\n",
    "                lm.set_depth(camera.get_depth(int(lm.x*IMAGE_WIDTH), int(lm.y*IMAGE_HEIGHT)))\n",
    "            hand_landmarks_list.append(lm)\n",
    "        else:\n",
    "            hand_landmarks_list.append(LocalLandmark(0, 0, 0, 0))\n",
    "                \n",
    "    return hand_landmarks_list\n",
    "\n",
    "def draw_landmarks(frame, landmarks, side, connections):\n",
    "    \"\"\"side: 0 = right, 1 = left\"\"\"\n",
    "\n",
    "    if all([lm.is_empty() for lm in landmarks]):\n",
    "        return\n",
    "\n",
    "    right_landmarks_color = (0, 0, 255)\n",
    "    left_landmarks_color = (255, 0, 0)\n",
    "\n",
    "    for lm in landmarks:\n",
    "        if side == 0:\n",
    "            landmarks_color = right_landmarks_color\n",
    "        elif side == 1:\n",
    "            landmarks_color = left_landmarks_color\n",
    "        else:\n",
    "            return\n",
    "        cv2.circle(frame, (int(lm.x*IMAGE_WIDTH), int(lm.y*IMAGE_HEIGHT)), 2, landmarks_color, 2)\n",
    "\n",
    "    for connection in connections:\n",
    "        if connection[1] < len(landmarks):\n",
    "            start_point = (int(landmarks[connection[0]].x*IMAGE_WIDTH), int(landmarks[connection[0]].y*IMAGE_HEIGHT))\n",
    "            end_point = (int(landmarks[connection[1]].x*IMAGE_WIDTH), int(landmarks[connection[1]].y*IMAGE_HEIGHT))\n",
    "            cv2.line(frame, start_point, end_point, (255, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_probability_bars(frame, probabilities):\n",
    "    bar_colors = ((255, 0 , 0), (0, 255, 0), (0, 0, 255), (255, 255, 0))\n",
    "    for i, action_prob in enumerate(probabilities):\n",
    "        cv2.rectangle(frame, (0, 35 + i*20), (int(action_prob*100), 55 + i*20), bar_colors[i], -1)\n",
    "        label_color = (0, 255, 255) if action_prob >= 0.9 else (255, 255, 255)\n",
    "        cv2.putText(frame, f\"{Action(i).name.lower()}: {(action_prob*100):.2f} %\", (110, 50 + i*20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, label_color, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialSmoothing:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.smoothed_probabilities = None\n",
    "\n",
    "    def update(self, probabilities):\n",
    "        if self.smoothed_probabilities is None:\n",
    "            self.smoothed_probabilities = probabilities\n",
    "        else:\n",
    "            self.smoothed_probabilities = self.alpha*probabilities + (1 - self.alpha)*self.smoothed_probabilities\n",
    "        return self.smoothed_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models (TorchScript)\n",
    "\n",
    "# Action recognition\n",
    "model_ar = torch.jit.load(os.path.join(\"weights\", \"model_ar_simple_lstm.pt\"))\n",
    "model_ar.eval()\n",
    "\n",
    "# Action prediction\n",
    "model_ap = torch.jit.load(os.path.join(\"weights\", \"model_ap.pt\"))\n",
    "model_ap.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_action(data_buffer, smoother=None):\n",
    "    X = torch.tensor(np.array(data_buffer), dtype=torch.float32).to(device)\n",
    "    X = torch.unsqueeze(X, axis=0)\n",
    "    with torch.no_grad():\n",
    "        probabilities = F.softmax(model_ar(X)[0], dim=0)\n",
    "        if smoother is not None:\n",
    "            return smoother.update(probabilities)\n",
    "        else:\n",
    "            return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_action_in_sequence(action_index, currently_in_hand_tracked_objects, actions_list, right_hand_objects_list, left_hand_objects_list):\n",
    "    right_hand_object_index = currently_in_hand_tracked_objects[0].yolo_object.label_id if currently_in_hand_tracked_objects[0] is not None else len(OBJECT_NAMES)\n",
    "    left_hand_object_index = currently_in_hand_tracked_objects[1].yolo_object.label_id if currently_in_hand_tracked_objects[1] is not None else len(OBJECT_NAMES)\n",
    "    print(\"Registering new action:\", action_index, right_hand_object_index) # left_hand_object_index\n",
    "    actions_list.append(action_index)\n",
    "    right_hand_objects_list.append(right_hand_object_index)\n",
    "    left_hand_objects_list.append(left_hand_object_index)\n",
    "\n",
    "def one_hot_encode_class(class_index, num_classes):\n",
    "    one_hot_vector = np.zeros(num_classes, dtype=int)\n",
    "    one_hot_vector[class_index] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def predict_next_action(one_hot_sequence_data):\n",
    "    \"\"\"\n",
    "    Predict the next most probable action based on provided one-hot sequence data.\n",
    "    Returns the probabilities for predicted actions and objects.\n",
    "    \"\"\"\n",
    "    X = torch.tensor(np.array(one_hot_sequence_data), dtype=torch.float32).to(device)\n",
    "    X = torch.unsqueeze(X, axis=0)\n",
    "    with torch.no_grad():\n",
    "        outputs1, outputs2 = model_ap(X)\n",
    "        prob1 = F.softmax(outputs1, dim=1)[0]\n",
    "        prob2 = F.softmax(outputs2, dim=1)[0]\n",
    "        return prob1, prob2\n",
    "\n",
    "def search_for_future_action(sequence_data, action_to_find, force_object=False):\n",
    "    \"\"\"\n",
    "    Uses action prediction to search for a specified future action.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence_data: numpy.ndarray, shape (sequence_length, 2)\n",
    "            Sequence data in pairs (tuple): (action_index, object_index).\n",
    "        action_to_find: Action\n",
    "            Action to search for in the future.\n",
    "        force_object: bool, default False\n",
    "            If True, if the object found for the specified action is none,\n",
    "            the second most probable object will be returned.\n",
    "    \n",
    "    Returns:\n",
    "        prediction_pairs: list\n",
    "            List of predicted pairs from the next step to the action to find.\n",
    "        conf: tuple (action_confidence, object_confidence)\n",
    "            Confidence in the action to be found, calculated as the conditional probability of \n",
    "            the previous action confidences in the prediction_pairs (multiplied confidences),\n",
    "            ignoring idle actions, and the confidence of the relative predicted object.\n",
    "        object_forced: bool\n",
    "            Whether the object found for the specified action was forced.\n",
    "            It always returns False if force_object is False.\n",
    "    \"\"\"\n",
    "    one_hot_sequence_data = deque(maxlen=PREDICTION_WINDOW_SIZE)\n",
    "    for action_pair in sequence_data:\n",
    "        one_hot_sequence_data.append(np.concatenate((one_hot_encode_class(action_pair[0], num_classes_actions), one_hot_encode_class(action_pair[1], num_classes_objects)), axis=0))\n",
    "\n",
    "    prediction_pairs = []\n",
    "    confidences = []\n",
    "    object_forced = False\n",
    "    while True:\n",
    "        prob1, prob2 = predict_next_action(one_hot_sequence_data)\n",
    "        action_prediction_index = torch.argmax(prob1).item()\n",
    "        object_prediction_indices = torch.sort(prob2, descending=True).indices.tolist()\n",
    "        \n",
    "        if Action(action_prediction_index) == action_to_find: # Action found\n",
    "            if force_object and object_prediction_indices[0] == len(OBJECT_NAMES): # Object forced\n",
    "                prediction_pairs.append((action_prediction_index, object_prediction_indices[1]))\n",
    "                confidences.append((prob1[action_prediction_index].item(), prob2[object_prediction_indices[1]].item()))\n",
    "                object_forced = True\n",
    "            else: # Object not forced\n",
    "                prediction_pairs.append((action_prediction_index, object_prediction_indices[0]))\n",
    "                confidences.append((prob1[action_prediction_index].item(), prob2[object_prediction_indices[0]].item()))\n",
    "            break\n",
    "        else: # Add predicted row to sequence and predict next\n",
    "            action_pair = (action_prediction_index, object_prediction_indices[0])\n",
    "            prediction_pairs.append(action_pair)\n",
    "            confidences.append((prob1[action_pair[0]].item(), prob2[action_pair[1]].item()))\n",
    "            one_hot_sequence_data.append(np.concatenate((one_hot_encode_class(action_pair[0], num_classes_actions), one_hot_encode_class(action_pair[1], num_classes_objects)), axis=0))\n",
    "\n",
    "    action_conditional_probability = 1.0\n",
    "    for i in range(len(confidences)):\n",
    "        if prediction_pairs[i][0] != Action.IDLE.value:\n",
    "            action_conditional_probability *= confidences[i][0]\n",
    "\n",
    "    return prediction_pairs, tuple((action_conditional_probability, confidences[-1][1])), object_forced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_and_deliver_object(hand_helper, object_tracker, camera, robot_controller, actions_list, right_hand_objects_list, left_hand_objects_list, execution_queue):\n",
    "    robot_controller.is_moving = True\n",
    "    while execution_queue:\n",
    "        pick_action_pair = execution_queue.pop(0)\n",
    "        to_pick_object_label_id = pick_action_pair[1]\n",
    "        to_pick_tracked_object = None\n",
    "        for tracked_object in object_tracker.tracked_objects:\n",
    "            if tracked_object.yolo_object.label_id == to_pick_object_label_id and tracked_object.is_visible and not tracked_object.is_moving \\\n",
    "               and not (object_tracker.right_hand_tracked_object and object_tracker.right_hand_tracked_object.tracker_id == tracked_object.tracker_id) \\\n",
    "               and not (object_tracker.left_hand_tracked_object and object_tracker.left_hand_tracked_object.tracker_id == tracked_object.tracker_id):\n",
    "                cx, cy = tracked_object.yolo_object.get_center()\n",
    "                depth = camera.get_depth(cx, cy)\n",
    "                P_cam = np.array(camera.deproject_pixel_to_point(cx, cy, depth) + [1])\n",
    "                P_rob = robot_controller.cam_to_robot_transform(P_cam)\n",
    "                x, y = P_rob[0], P_rob[1]\n",
    "                z_approach = 100.0\n",
    "                z = 25.0\n",
    "                if robot_controller.check_workspace(x, y, z_approach) and robot_controller.check_workspace(x, y, z):\n",
    "                    to_pick_tracked_object = tracked_object\n",
    "                    break\n",
    "        # Picking\n",
    "        if to_pick_tracked_object is not None:\n",
    "            z_shift = z - z_approach\n",
    "            approaching_speed = 130 if robot_controller.is_at_home_pos else 80\n",
    "            robot_controller.move_cartesian(x=x, y=y, z=z_approach, speed=approaching_speed)\n",
    "            robot_controller.shift_cartesian_rel_base(z_shift=z_shift, speed=80)\n",
    "            robot_controller.is_delivering = True\n",
    "            robot_controller.close_gripper()\n",
    "            robot_controller.shift_cartesian_rel_base(z_shift=-z_shift, speed=80)\n",
    "            # Delivering\n",
    "            delivered = False\n",
    "            while not delivered and robot_controller.is_connected:\n",
    "                right_hand_center = hand_helper.get_hand_centers()[0]\n",
    "                cx, cy = right_hand_center[0], right_hand_center[1]\n",
    "                if cx > 0 and cy > 0:\n",
    "                    depth = camera.get_depth(cx, cy)\n",
    "                    P_cam = np.array(camera.deproject_pixel_to_point(cx, cy, depth) + [1])\n",
    "                    P_rob = robot_controller.cam_to_robot_transform(P_cam)\n",
    "                    x_hand, y_hand, z_hand = P_rob[0], P_rob[1], P_rob[2]\n",
    "                    z_hand += 50\n",
    "                    current_robot_pose = robot_controller.get_current_pose_cartesian()\n",
    "                    distance = (x_hand - current_robot_pose[0])**2 + (y_hand - current_robot_pose[1])**2 + (z_hand - current_robot_pose[2])**2\n",
    "                    distance_threshold = 1600\n",
    "                    if distance <= distance_threshold:\n",
    "                        robot_controller.open_gripper()\n",
    "                        delivered = True\n",
    "                        robot_controller.is_delivering = False\n",
    "                        object_tracker.force_object_in_hand(0, to_pick_tracked_object)\n",
    "                        register_action_in_sequence(Action.PICK.value, (to_pick_tracked_object, None), actions_list, right_hand_objects_list, left_hand_objects_list)\n",
    "                        sequence_data = list(zip(actions_list, right_hand_objects_list))\n",
    "                        if len(sequence_data) >= PREDICTION_WINDOW_SIZE:\n",
    "                            prediction_pairs, conf, object_forced = search_for_future_action(sequence_data[-PREDICTION_WINDOW_SIZE:], Action.PICK, force_object=True)\n",
    "                            print(\"Prediction\", prediction_pairs, conf, object_forced)\n",
    "                            # Action anticipation logic\n",
    "                            pick_action_pair = prediction_pairs[-1]\n",
    "                            if pick_action_pair[0] == Action.PICK.value: # Redundant check\n",
    "                                if conf[0] > 0.8 and conf[1] > 0.6:\n",
    "                                    execution_queue.append(pick_action_pair)\n",
    "                                else:\n",
    "                                    print(f\"Action {pick_action_pair} not executed for not enough confidence: {conf}\")\n",
    "                    elif robot_controller.check_workspace(x_hand, y_hand, z_hand):\n",
    "                        robot_controller.move_cartesian(x=x_hand, y=y_hand, z=z_hand, speed=80)\n",
    "                time.sleep(0.3)\n",
    "\n",
    "        else:\n",
    "            print(f\"No object with label {to_pick_object_label_id} found in workspace\")\n",
    "\n",
    "        if not execution_queue and not robot_controller.is_at_home_pos:\n",
    "            robot_controller.move_to_home_pose()\n",
    "    \n",
    "    robot_controller.is_moving = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = RealSenseCamera(image_width=IMAGE_WIDTH, image_height=IMAGE_HEIGHT)\n",
    "camera.connect()\n",
    "\n",
    "robot_controller = RobotController()\n",
    "robot_controller.connect()\n",
    "robot_controller.move_to_home_pose()\n",
    "robot_controller.open_gripper()\n",
    "\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "first_frame_acquisition_time = 0\n",
    "frame_index = 0\n",
    "\n",
    "data_buffer = deque(maxlen=sequence_length)\n",
    "action_recognition_probabilities = None\n",
    "\n",
    "hand_helper = HandHelper(image_width=IMAGE_WIDTH, image_height=IMAGE_HEIGHT)\n",
    "object_tracker = ObjectTracker(image_width=IMAGE_WIDTH, image_height=IMAGE_HEIGHT)\n",
    "\n",
    "smoother = ExponentialSmoothing(alpha=0.4)\n",
    "\n",
    "current_top_action_index = -1\n",
    "actions_list = []\n",
    "right_hand_objects_list = []\n",
    "left_hand_objects_list = []\n",
    "execution_queue = []\n",
    "\n",
    "try:\n",
    "    with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "        while True:\n",
    "            ret = camera.acquire_frame()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            color_image = camera.color_image\n",
    "\n",
    "            hands_results = process_landmarks(color_image, hands)\n",
    "            objects_results = yolo_model(color_image, verbose=False)[0]\n",
    "\n",
    "            new_frame_time = time.time()\n",
    "            fps = 1.0/(new_frame_time - prev_frame_time)\n",
    "            prev_frame_time = new_frame_time\n",
    "            cv2.putText(color_image, f\"{fps:.2f}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 1)\n",
    "\n",
    "            right_hand_landmarks = extract_hand_landmarks(0, hands_results.multi_hand_landmarks, hands_results.multi_handedness)\n",
    "            left_hand_landmarks = extract_hand_landmarks(1, hands_results.multi_hand_landmarks, hands_results.multi_handedness)\n",
    "            hand_helper.register_hands_landmarks(right_hand_landmarks, left_hand_landmarks)\n",
    "            tips_midpoints = hand_helper.get_tips_midpoints()\n",
    "            hand_centers = hand_helper.get_hand_centers()\n",
    "\n",
    "            draw_landmarks(color_image, right_hand_landmarks, side=0, connections=HAND_CONNECTIONS)\n",
    "            draw_landmarks(color_image, left_hand_landmarks, side=1, connections=HAND_CONNECTIONS)\n",
    "            # cv2.circle(color_image, tips_midpoints[0], 5, (0, 255, 0), -1)\n",
    "            # cv2.circle(color_image, tips_midpoints[1], 5, (0, 255, 0), -1)\n",
    "            # cv2.circle(color_image, hand_centers[0], 5, (0, 255, 255), -1)\n",
    "            # cv2.circle(color_image, hand_centers[1], 5, (0, 255, 255), -1)\n",
    "\n",
    "            seen_yolo_objects = []\n",
    "            for object_result in objects_results.boxes:\n",
    "                if object_result.conf.item() > 0.75:\n",
    "                    seen_yolo_objects.append(YoloObject.from_yolo_box_result(object_result))\n",
    "\n",
    "            object_tracker.register_seen_objects(seen_yolo_objects, tips_midpoints)\n",
    "            object_tracker.increment_frame_index()\n",
    "            \n",
    "            for i, in_hand_tracked_object in enumerate((object_tracker.right_hand_tracked_object, object_tracker.left_hand_tracked_object)):\n",
    "                if in_hand_tracked_object is None:\n",
    "                    continue\n",
    "                hand_color = (0, 0, 255) if i == 0 else (255, 0, 0)\n",
    "                yolo_object = in_hand_tracked_object.yolo_object\n",
    "                cv2.putText(color_image, f\"{OBJECT_NAMES[yolo_object.label_id]}, visible: {in_hand_tracked_object.is_visible}\", (10, 140 + i*25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, hand_color, 1)\n",
    "                if in_hand_tracked_object.is_visible:\n",
    "                    cv2.rectangle(color_image, (yolo_object.x1, yolo_object.y1), (yolo_object.x2, yolo_object.y2), hand_color, 2)\n",
    "                    cv2.putText(color_image, f\"{OBJECT_NAMES[yolo_object.label_id]}, conf: {yolo_object.conf:.2f}\",\n",
    "                                (yolo_object.x1, yolo_object.y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, hand_color, 1)\n",
    "            \n",
    "            # Action recognition logic\n",
    "            elapsed_time = time.time() - first_frame_acquisition_time - frame_index*frame_interval\n",
    "            if elapsed_time > frame_interval:\n",
    "                hands_landmarks = right_hand_landmarks + left_hand_landmarks \n",
    "                frame_landmarks_data = np.concatenate([lm.get_np_array() for lm in hands_landmarks])\n",
    "                \"\"\" # Also include objects in model input\n",
    "                frame_objects_data = []\n",
    "                for i, in_hand_tracked_object in enumerate((object_tracker.right_hand_tracked_object, object_tracker.left_hand_tracked_object)):\n",
    "                    if in_hand_tracked_object is None:\n",
    "                        continue\n",
    "                    yolo_object = in_hand_tracked_object.yolo_object\n",
    "                    frame_objects_data.append(np.array((yolo_object.label_id, i, yolo_object.x1, yolo_object.y1, yolo_object.x2, yolo_object.y2))) \"\"\"\n",
    "                selected_frame_data = get_selected_frame_data(frame_landmarks_data, None)\n",
    "                data_buffer.append(selected_frame_data)\n",
    "                if len(data_buffer) == sequence_length:\n",
    "                    action_recognition_probabilities = recognize_action(data_buffer, smoother=smoother)\n",
    "                    best_action_index = torch.argmax(action_recognition_probabilities).item()\n",
    "                    # Action prediction logic (looking for next PICK action)\n",
    "                    if action_recognition_probabilities[best_action_index] > 0.9 \\\n",
    "                       and not (actions_list and actions_list[-1] == best_action_index) \\\n",
    "                       and current_top_action_index != best_action_index:\n",
    "                        current_top_action_index = best_action_index\n",
    "                        if not robot_controller.is_delivering: # Not registering in sequence if the robot is delivering\n",
    "                            register_action_in_sequence(best_action_index, (object_tracker.right_hand_tracked_object, object_tracker.left_hand_tracked_object), actions_list, right_hand_objects_list, left_hand_objects_list)\n",
    "                            if not robot_controller.is_moving or robot_controller.is_at_home_pos: # Not predicting if the robot is moving\n",
    "                                sequence_data = list(zip(actions_list, right_hand_objects_list))\n",
    "                                if len(sequence_data) >= PREDICTION_WINDOW_SIZE:\n",
    "                                    prediction_pairs, conf, object_forced = search_for_future_action(sequence_data[-PREDICTION_WINDOW_SIZE:], Action.PICK, force_object=True)\n",
    "                                    print(\"Prediction\", prediction_pairs, conf, object_forced)\n",
    "                                    # Action anticipation logic\n",
    "                                    pick_action_pair = prediction_pairs[-1]\n",
    "                                    if pick_action_pair[0] == Action.PICK.value: # Redundant check\n",
    "                                        if conf[0] > 0.8 and conf[1] > 0.6:\n",
    "                                            execution_queue.append(pick_action_pair)\n",
    "                                            if not robot_controller.is_moving:\n",
    "                                                threading.Thread(target=pick_and_deliver_object, args=(hand_helper, object_tracker, camera, robot_controller, actions_list, right_hand_objects_list, left_hand_objects_list, execution_queue)).start()\n",
    "                                        else:\n",
    "                                            print(f\"Action {pick_action_pair} not executed for not enough confidence: {conf}\")\n",
    "\n",
    "                if frame_index == 0:\n",
    "                    first_frame_acquisition_time = time.time()\n",
    "                    \n",
    "                frame_index += 1\n",
    "\n",
    "            if action_recognition_probabilities is not None:\n",
    "                show_probability_bars(color_image, action_recognition_probabilities)\n",
    "            \n",
    "            cv2.imshow(\"RealSense\", color_image)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    camera.disconnect()\n",
    "    robot_controller.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actions_list, len(actions_list))\n",
    "print(right_hand_objects_list, len(right_hand_objects_list))\n",
    "print(left_hand_objects_list, len(left_hand_objects_list))\n",
    "for i in range(len(actions_list)):\n",
    "    action_name = Action(actions_list[i]).name.lower()\n",
    "    right_hand_object_name = OBJECT_NAMES[right_hand_objects_list[i]] if right_hand_objects_list[i] < len(OBJECT_NAMES) else \"none\"\n",
    "    # left_hand_object_name = OBJECT_NAMES[left_hand_objects_list[i]] if left_hand_objects_list[i] < len(OBJECT_NAMES) else \"none\"\n",
    "    print(f\"Action: {action_name}, Right: {right_hand_object_name}\") # , Left: {left_hand_object_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_observation(actions_list, right_hand_objects_list, left_hand_objects_list): # Now just considering right hand\n",
    "    OBSERVATIONS_FOLDER = \"observations\"\n",
    "\n",
    "    observation_number = 0\n",
    "    if os.path.exists(OBSERVATIONS_FOLDER):\n",
    "        observation_names = os.listdir(OBSERVATIONS_FOLDER)\n",
    "        if observation_names:\n",
    "            observation_numbers = []\n",
    "            for observation_name in observation_names:\n",
    "                observation_numbers.append(int(observation_name.split('.')[0].split('_')[1]))\n",
    "            observation_number = sorted(observation_numbers, reverse=True)[0] + 1\n",
    "\n",
    "    observation_path = os.path.join(OBSERVATIONS_FOLDER, f\"observation_{observation_number}.csv\")\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(actions_list)):\n",
    "        row_dict = {\"action\": actions_list[i], \"right_hand_object\": right_hand_objects_list[i]}\n",
    "        rows.append(row_dict)\n",
    "        \n",
    "    pd.DataFrame(rows).to_csv(observation_path, encoding=\"utf-8\", index=False)\n",
    "    print(f\"Saved new observation (just right hand) at {observation_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_observation(actions_list, right_hand_objects_list, left_hand_objects_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
