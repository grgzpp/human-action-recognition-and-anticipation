{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from enum import Enum\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from torchinfo import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from local_landmark import LocalLandmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "SAMPLING_FPS = 10\n",
    "SAMPLING_SECONDS = 1\n",
    "sequence_length = int(SAMPLING_SECONDS*SAMPLING_FPS)\n",
    "\n",
    "DATA_FOLDER = \"sample_data\"\n",
    "\n",
    "IMAGE_WIDTH = 640\n",
    "IMAGE_HEIGHT = 480\n",
    "\n",
    "HAND_CONNECTIONS = ((0, 1), (0, 5), (9, 13), (13, 17), (5, 9), (0, 17), (1, 2), (2, 3), (3, 4), (5, 6), (6, 7), (7, 8),\n",
    "                    (9, 10), (10, 11), (11, 12), (13, 14), (14, 15), (15, 16), (17, 18), (18, 19), (19, 20))\n",
    "\n",
    "class Action(Enum):\n",
    "    IDLE = 0\n",
    "    PICK = 1\n",
    "    PLACE = 2\n",
    "    SCREW_WRENCH = 3\n",
    "\n",
    "OBJECT_NAMES = {\n",
    "    0: \"small_screw\",\n",
    "    1: \"big_screw\",\n",
    "    2: \"small_wrench\",\n",
    "    3: \"big_wrench\",\n",
    "    4: \"cap\",\n",
    "    5: \"barrel\",\n",
    "    6: \"piston\",\n",
    "    7: \"support\",\n",
    "    8: \"air_connector\",\n",
    "    9: \"nut\"\n",
    "}\n",
    "\n",
    "num_classes_actions = len(Action)\n",
    "num_classes_objects = len(OBJECT_NAMES) + 1 # Last class for empty hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks_from_flattened_array(flattened_landmarks):\n",
    "    N_RIGHT_HAND_LANDMARKS = 21\n",
    "    N_LEFT_HAND_LANDMARKS = 21\n",
    "\n",
    "    right_hand_landmarks = []\n",
    "    left_hand_landmarks = []\n",
    "\n",
    "    cursor = 0\n",
    "\n",
    "    for _ in range(N_RIGHT_HAND_LANDMARKS):\n",
    "        cursor_end_position = cursor + 4\n",
    "        lm_sub_array = flattened_landmarks[cursor:cursor_end_position]\n",
    "        right_hand_landmarks.append(LocalLandmark.from_np_array(lm_sub_array))\n",
    "        cursor = cursor_end_position\n",
    "\n",
    "    for _ in range(N_LEFT_HAND_LANDMARKS):\n",
    "        cursor_end_position = cursor + 4\n",
    "        lm_sub_array = flattened_landmarks[cursor:cursor_end_position]\n",
    "        left_hand_landmarks.append(LocalLandmark.from_np_array(lm_sub_array))\n",
    "        cursor = cursor_end_position\n",
    "\n",
    "    return right_hand_landmarks, left_hand_landmarks\n",
    "\n",
    "def one_hot_encode_class(class_index, num_classes):\n",
    "    one_hot_vector = np.zeros(num_classes, dtype=int)\n",
    "    one_hot_vector[class_index] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def get_selected_frame_data(frame_landmarks_data, frame_objects_data):\n",
    "    selected_frame_data = []\n",
    "\n",
    "    right_hand_landmarks, left_hand_landmarks = get_landmarks_from_flattened_array(frame_landmarks_data)\n",
    "    for i in range(2):\n",
    "        if i == 0:\n",
    "            hand_landmarks = right_hand_landmarks\n",
    "        else:\n",
    "            continue # Select only the right hand\n",
    "            hand_landmarks = left_hand_landmarks\n",
    "            \n",
    "        wrist_landmark = hand_landmarks[0]\n",
    "        for lm in hand_landmarks:\n",
    "            # Absolute coordinates (with respect to camera frame)\n",
    "            selected_frame_data.append(lm.x)\n",
    "            selected_frame_data.append(lm.y)\n",
    "            # selected_frame_data.append(lm.d/IMAGE_WIDTH)\n",
    "\n",
    "            # Relative coordinates (relative to the wrist landmark)\n",
    "            selected_frame_data.append(lm.x - wrist_landmark.x)\n",
    "            selected_frame_data.append(lm.y - wrist_landmark.y)\n",
    "            selected_frame_data.append(lm.z)\n",
    "\n",
    "    if frame_objects_data is not None:\n",
    "        right_hand_object_found = False\n",
    "        for frame_object_data in frame_objects_data:\n",
    "            label_id, in_hand, x1, y1, x2, y2 = frame_object_data\n",
    "            if in_hand == 0: # Only objects held in the right hand are considered\n",
    "                selected_frame_data = np.concatenate((selected_frame_data, one_hot_encode_class(label_id, num_classes_objects)), axis=0)\n",
    "                right_hand_object_found = True\n",
    "                break\n",
    "        \n",
    "        if not right_hand_object_found:\n",
    "            selected_frame_data = np.concatenate((selected_frame_data, one_hot_encode_class(num_classes_objects - 1, num_classes_objects)), axis=0)\n",
    "\n",
    "    return selected_frame_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None):\n",
    "    y = np.array(y, dtype=int)\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    categorical = np.zeros((len(y), num_classes), dtype=int)\n",
    "    categorical[np.arange(len(y)), y] = 1\n",
    "    return categorical\n",
    "\n",
    "def train_val_test_split(X_data, y_data, val_size=0.1, test_size=0.1, shuffle=True, random_state=None):\n",
    "    assert len(X_data) == len(y_data), \"The first dimension of X_data and y_data must coincide\"\n",
    "    assert val_size + test_size < 1.0, \"The sum of val_size and test_size must be less than 1.0\"\n",
    "\n",
    "    n_samples = len(X_data)\n",
    "\n",
    "    if shuffle:\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_data = X_data[indices]\n",
    "        y_data = y_data[indices]\n",
    "    \n",
    "    train_size = 1.0 - val_size - test_size\n",
    "    train_end = int(train_size*n_samples)\n",
    "    val_end = train_end + int(val_size*n_samples)\n",
    "    \n",
    "    X_train = X_data[:train_end]\n",
    "    y_train = y_data[:train_end]\n",
    "    X_val = X_data[train_end:val_end]\n",
    "    y_val = y_data[train_end:val_end]\n",
    "    X_test = X_data[val_end:]\n",
    "    y_test = y_data[val_end:]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_data = []\n",
    "objects_data = []\n",
    "labels = []\n",
    "for action in list(Action):\n",
    "    action_name = action.name.lower()\n",
    "    print(\"Loading sequences data for action:\", action_name)\n",
    "    action_folder = os.path.join(DATA_FOLDER, action_name)\n",
    "    for sequence_data_folder in tqdm(os.listdir(action_folder)):\n",
    "        sequence_data_landmarks_folder = os.path.join(action_folder, sequence_data_folder, \"landmarks\")\n",
    "        sequence_data_objects_folder = os.path.join(action_folder, sequence_data_folder, \"objects\")\n",
    "        sequence_landmarks_data = []\n",
    "        sequence_objects_data = []\n",
    "        for frame_index in range(sequence_length):\n",
    "            frame_landmarks_data = np.load(os.path.join(sequence_data_landmarks_folder, f\"frame_{frame_index}.npy\"))\n",
    "            sequence_landmarks_data.append(frame_landmarks_data)\n",
    "            frame_objects_data = np.load(os.path.join(sequence_data_objects_folder, f\"frame_{frame_index}.npy\"))\n",
    "            sequence_objects_data.append(frame_objects_data)\n",
    "\n",
    "        landmarks_data.append(sequence_landmarks_data)\n",
    "        objects_data.append(sequence_objects_data)\n",
    "        labels.append(action.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select actions data NO objects\n",
    "selected_actions_data = []\n",
    "for sequence_landmarks_data in landmarks_data:\n",
    "    selected_sequence_data = []\n",
    "    for frame_landmarks_data in sequence_landmarks_data:\n",
    "        selected_frame_data = get_selected_frame_data(frame_landmarks_data, None)\n",
    "        selected_sequence_data.append(selected_frame_data)\n",
    "\n",
    "    selected_actions_data.append(selected_sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select actions data WITH objects\n",
    "selected_actions_data = []\n",
    "for sequence_index in range(len(landmarks_data)):\n",
    "    sequence_landmarks_data = landmarks_data[sequence_index]\n",
    "    sequence_objects_data = objects_data[sequence_index]\n",
    "    selected_sequence_data = []\n",
    "    for frame_index in range(len(sequence_landmarks_data)):\n",
    "        frame_landmarks_data = sequence_landmarks_data[frame_index]\n",
    "        frame_objects_data = sequence_objects_data[frame_index]\n",
    "        selected_frame_data = get_selected_frame_data(frame_landmarks_data, frame_objects_data)\n",
    "        selected_sequence_data.append(selected_frame_data)\n",
    "\n",
    "    selected_actions_data.append(selected_sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array(selected_actions_data)\n",
    "y_data = to_categorical(labels, num_classes=num_classes_actions)\n",
    "print(\"X shape:\", X_data.shape)\n",
    "print(\"y shape:\", y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X_data, y_data, val_size=0.1, test_size=0.1, random_state=RANDOM_SEED)\n",
    "print(\"X shape (train, validation, test):\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"y shape (train, validation, test):\", y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexLSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n",
    "        super(ComplexLSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = F.relu(self.fc1(out[:, -1, :]))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.shape[0], self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.shape[0], self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DNetClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout=0.2):\n",
    "        super(Conv1DNetClassifier, self).__init__()\n",
    "        self.conv1d_1 = nn.Conv1d(input_size, hidden_size, kernel_size=3)\n",
    "        self.conv1d_2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=2)\n",
    "        self.conv1d_3 = nn.Conv1d(hidden_size, hidden_size, kernel_size=2)\n",
    "        \n",
    "        self.maxpool1d = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.permute(x, dims=(0, 2, 1))\n",
    "        out = F.relu(self.conv1d_1(x))\n",
    "        out = self.maxpool1d(out)\n",
    "        out = F.relu(self.conv1d_2(out))\n",
    "        out = F.relu(self.conv1d_3(out))\n",
    "        out = torch.mean(out, dim=2)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMObjectsClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, num_layers, num_classes, dropout=0.2):\n",
    "        super(LSTMObjectsClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(105, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 20)\n",
    "        self.fc_extra1 = nn.Linear(31, 32)\n",
    "        self.fc_extra2 = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, x):\n",
    "        landmarks_data = x[:, :, :105]\n",
    "        objects_data = x[:, :, 105:]\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(landmarks_data, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = F.relu(self.fc(out))\n",
    "        out = F.relu(self.fc_extra1(torch.cat((out, torch.mean(objects_data, dim=1)), dim=1)))\n",
    "        out = self.fc_extra2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "input_size = X_train.shape[2]\n",
    "sequence_length = X_train.shape[1]\n",
    "initial_learning_rate = 0.001\n",
    "num_layers = 3\n",
    "hidden_size = 128\n",
    "num_classes = y_train.shape[1] # num_classes_actions\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "# model = ComplexLSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "# model = BiLSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "# model = Conv1DNetClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "# model = LSTMObjectsClassifier(hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_learning_rate)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[40, 80], gamma=0.1)\n",
    "\n",
    "summary(model, input_size=(16, sequence_length, input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_enabled = True\n",
    "patience = 15\n",
    "min_delta_improvement = 0.001\n",
    "restore_best_weights = True\n",
    "\n",
    "best_loss = np.Inf\n",
    "counter = 0\n",
    "best_epoch = 0\n",
    "\n",
    "if early_stopping_enabled and restore_best_weights:\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}, lr: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if early_stopping_enabled:\n",
    "        if val_loss < best_loss - min_delta_improvement:\n",
    "            best_loss = val_loss\n",
    "            counter = 0\n",
    "            best_epoch = epoch + 1\n",
    "            if restore_best_weights:\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            restore_best_weights_message = f\" Best weights restored to epoch {best_epoch}.\" if restore_best_weights else \"\"\n",
    "            print(f\"Early stopping at epoch {epoch + 1} with no improvement in validation loss.{restore_best_weights_message}\")\n",
    "            break\n",
    "\n",
    "# Restore best weights\n",
    "if early_stopping_enabled and restore_best_weights:\n",
    "    model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state and optimizer state\n",
    "model_checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "torch.save(model_checkpoint, \"model_ar.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model state and optimizer state\n",
    "model_checkpoint = torch.load(\"model_ar.pth\")\n",
    "model.load_state_dict(model_checkpoint[\"state_dict\"])\n",
    "optimizer.load_state_dict(model_checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model TorchScript\n",
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save(os.path.join(\"weights\", \"model_ar.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (Cross-Entropy)\")\n",
    "plt.ylim([-0.05, 1.4])\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, normalize=False):\n",
    "    if normalize:\n",
    "        cm = cm/cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(np.transpose(cm), annot=True, fmt='.2f' if normalize else 'd', cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"True\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_performance_metrics(y_true, outputs, labels):\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, preds)\n",
    "    precision = precision_score(y_true, preds, average=\"macro\")\n",
    "    recall = recall_score(y_true, preds, average=\"macro\")\n",
    "    f1 = f1_score(y_true, preds, average=\"macro\")\n",
    "    conf_matrix = confusion_matrix(y_true, preds)\n",
    "    roc_auc = roc_auc_score(y_true, outputs.cpu().numpy(), multi_class=\"ovr\")\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    plot_confusion_matrix(conf_matrix, labels, normalize=True)\n",
    "\n",
    "def ROC_curve(y_true, outputs, labels):\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    n_classes = outputs.size(1)\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true == i, outputs[:, i].cpu().numpy())\n",
    "        roc_auc[i] = roc_auc_score(y_true == i, outputs[:, i].cpu().numpy())\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, label=f\"ROC curve of action '{labels[i]}' (area = {roc_auc[i]:.2f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.legend(loc=\"lower left\", bbox_to_anchor=(0.4, 0.0))\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = F.softmax(model(X_val_tensor), dim=1)\n",
    "    test_outputs = F.softmax(model(X_test_tensor), dim=1)\n",
    "\n",
    "val_true = torch.argmax(y_val_tensor, dim=1).cpu().numpy()\n",
    "test_true = torch.argmax(y_test_tensor, dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set performance metrics\n",
    "evaluate_performance_metrics(val_true, val_outputs, [action.name.lower() for action in list(Action)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set performance metrics\n",
    "evaluate_performance_metrics(test_true, test_outputs, [action.name.lower() for action in list(Action)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set ROC curve\n",
    "ROC_curve(val_true, val_outputs, [action.name.lower() for action in list(Action)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set ROC curve\n",
    "ROC_curve(test_true, test_outputs, [action.name.lower() for action in list(Action)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(frame, landmarks, side, connections):\n",
    "    \"\"\"side: 0 = right, 1 = left\"\"\"\n",
    "\n",
    "    if all([lm.is_empty() for lm in landmarks]):\n",
    "        return\n",
    "    \n",
    "    image_height, image_width = frame.shape[0], frame.shape[1]\n",
    "\n",
    "    right_landmarks_color = (0, 0, 255)\n",
    "    left_landmarks_color = (255, 0, 0)\n",
    "\n",
    "    for lm in landmarks:\n",
    "        if side == 0:\n",
    "            landmarks_color = right_landmarks_color\n",
    "        elif side == 1:\n",
    "            landmarks_color = left_landmarks_color\n",
    "        else:\n",
    "            return\n",
    "        cv2.circle(frame, (int(lm.x*image_width), int(lm.y*image_height)), 2, landmarks_color, 2)\n",
    "\n",
    "    for connection in connections:\n",
    "        if connection[1] < len(landmarks):\n",
    "            start_point = (int(landmarks[connection[0]].x*image_width), int(landmarks[connection[0]].y*image_height))\n",
    "            end_point = (int(landmarks[connection[1]].x*image_width), int(landmarks[connection[1]].y*image_height))\n",
    "            cv2.line(frame, start_point, end_point, (255, 255, 255), 2)\n",
    "\n",
    "def get_blank_frame_with_landmarks(right_hand_landmarks, left_hand_landmarks, frame_counter=None, width=640, height=480):\n",
    "    blank_frame = np.zeros(shape=(height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    draw_landmarks(blank_frame, right_hand_landmarks, side=0, connections=HAND_CONNECTIONS)\n",
    "    draw_landmarks(blank_frame, left_hand_landmarks, side=1, connections=HAND_CONNECTIONS)\n",
    "\n",
    "    if frame_counter is not None:\n",
    "        cv2.putText(blank_frame, str(frame_counter), (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    return blank_frame\n",
    "\n",
    "def play_video_from_sequence_data(sequence_data):\n",
    "    frame_interval = 1.0/SAMPLING_FPS\n",
    "    \n",
    "    for frame_index, frame_data in enumerate(sequence_data):\n",
    "        frame_display_time = time.time()\n",
    "\n",
    "        right_hand_landmarks, left_hand_landmarks = get_landmarks_from_flattened_array(frame_data)\n",
    "        cv2.imshow(\"Video from sequence data\", get_blank_frame_with_landmarks(right_hand_landmarks, left_hand_landmarks, frame_counter=frame_index))\n",
    "\n",
    "        time_taken = time.time() - frame_display_time\n",
    "        time_to_wait = frame_interval - time_taken\n",
    "        if time_to_wait > 0:\n",
    "            time.sleep(time_to_wait)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data_path = os.path.join(DATA_FOLDER, Action.PLACE.name.lower(), \"sequence_37\")\n",
    "\n",
    "sequence_landmarks_data = []\n",
    "for frame_index in range(sequence_length):\n",
    "    frame_landmarks_data = np.load(os.path.join(sequence_data_path, \"landmarks\", f\"frame_{frame_index}.npy\"))\n",
    "    sequence_landmarks_data.append(frame_landmarks_data)\n",
    "\n",
    "# Video visualization\n",
    "play_video_from_sequence_data(sequence_landmarks_data)\n",
    "\n",
    "# Action recognition\n",
    "selected_sequence_data = []\n",
    "for frame_landmarks_data in sequence_landmarks_data:\n",
    "    selected_frame_data = get_selected_frame_data(frame_landmarks_data, None)\n",
    "    selected_sequence_data.append(selected_frame_data)\n",
    "X = torch.tensor(np.array(selected_sequence_data), dtype=torch.float32).to(device)\n",
    "X = torch.unsqueeze(X, axis=0)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    probabilities = F.softmax(model(X)[0], dim=0)\n",
    "print(\"Probabilities:\")\n",
    "for action_index, action_prob in enumerate(probabilities):\n",
    "    print(f\"- {Action(action_index).name.lower()}: {(action_prob*100):.2f} %\")\n",
    "action_recognition_prediction = Action(torch.argmax(probabilities).item()).name.lower()\n",
    "print(\"Action recognized:\", action_recognition_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
